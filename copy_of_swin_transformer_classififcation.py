# -*- coding: utf-8 -*-
"""Copy of SWIN_transformer_Classififcation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HQ4BnFucCa6GVvDjce07Tpq1Nj6bxyaX
"""

!pip install timm
import timm

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torchvision
from torch import nn, optim
from torchvision import transforms, datasets
from torch.utils.data import DataLoader
from google.colab import drive

device = "cuda" if torch.cuda.is_available() else "cpu"
device

drive.mount('/content/drive', force_remount=True)

train_dir = r'/content/drive/MyDrive/Colab Notebooks/Traintestuhh/train'
test_dir =r'/content/drive/MyDrive/Colab Notebooks/Traintestuhh/test'

NUM_WORKERS = os.cpu_count()

from torch.utils.data import random_split

def create_dataloaders(train_dir: str, test_dir: str, transform: transforms.Compose, batch_size: int, val_split: float = 0.2, num_workers: int = NUM_WORKERS):
    # Load the full training dataset
    full_train_data = datasets.ImageFolder(train_dir, transform=transform)

    # Calculate lengths for training and validation sets
    val_size = int(len(full_train_data) * val_split)
    train_size = len(full_train_data) - val_size

    # Split the dataset into training and validation sets
    train_data, val_data = random_split(full_train_data, [train_size, val_size])

    # Load the test dataset
    test_data = datasets.ImageFolder(test_dir, transform=transform)

    # Get class names
    class_names = full_train_data.classes

    # Create data loaders
    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

    return train_dataloader, val_dataloader, test_dataloader, class_names

IMG_SIZE = 224  # Correct image size for the Swin Transformer model
manual_transforms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

BATCH_SIZE = 32
train_dataloader, val_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=train_dir, test_dir=test_dir, transform=manual_transforms, batch_size=BATCH_SIZE)

print(len(class_names))
print(class_names)

image_batch, label_batch = next(iter(train_dataloader))
image, label = image_batch[0], label_batch[0]
print(image.shape, label)
plt.imshow(image.permute(1, 2, 0))
plt.title(class_names[label])
plt.axis(False);

total_images = len(train_dataloader.dataset)
print(f'Total number of training images: {total_images}')
val_images = len(val_dataloader.dataset)
print(f'Total number of validation images: {val_images}')
test_images = len(test_dataloader.dataset)
print(f'Total number of test images: {test_images}')

model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=len(class_names))
model = model.to(device)

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)

# Learning rate scheduler
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=7, verbose=True)

# Early Stopping class
class EarlyStopping:
    def __init__(self, patience=7, verbose=False):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.Inf

    def __call__(self, val_loss, model):
        score = -val_loss

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
        elif score < self.best_score:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model):
        '''Saves model when validation loss decreases.'''
        if self.verbose:
            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')
        torch.save(model.state_dict(), 'checkpoint.pt')
        self.val_loss_min = val_loss

early_stopping = EarlyStopping(patience=7, verbose=True)

# Training and evaluation loop with accuracy/loss calculation
def train_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, scheduler, num_epochs, early_stopping):
    train_losses = []
    val_losses = []
    test_losses = []
    train_accuracies = []
    val_accuracies = []
    test_accuracies = []

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        running_loss = 0.0
        correct_train = 0
        total_train = 0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs, 1)
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()

        epoch_train_loss = running_loss / len(train_loader.dataset)
        epoch_train_accuracy = correct_train / total_train
        train_losses.append(epoch_train_loss)
        train_accuracies.append(epoch_train_accuracy)
        print(f"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_train_loss:.4f}, Training Accuracy: {epoch_train_accuracy:.4f}")

        # Validation phase
        model.eval()
        val_loss = 0.0
        correct_val = 0
        total_val = 0

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs, 1)
                total_val += labels.size(0)
                correct_val += (predicted == labels).sum().item()

        epoch_val_loss = val_loss / len(val_loader.dataset)
        epoch_val_accuracy = correct_val / total_val
        val_losses.append(epoch_val_loss)
        val_accuracies.append(epoch_val_accuracy)
        print(f"Epoch {epoch+1}/{num_epochs}, Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {epoch_val_accuracy:.4f}")

        # Test phase
        test_loss = 0.0
        correct_test = 0
        total_test = 0

        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                test_loss += loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs, 1)
                total_test += labels.size(0)
                correct_test += (predicted == labels).sum().item()

        epoch_test_loss = test_loss / len(test_loader.dataset)
        epoch_test_accuracy = correct_test / total_test
        test_losses.append(epoch_test_loss)
        test_accuracies.append(epoch_test_accuracy)
        print(f"Epoch {epoch+1}/{num_epochs}, Testing Loss: {epoch_test_loss:.4f}, Testing Accuracy: {epoch_test_accuracy:.4f}")

        # Check for early stopping
        scheduler.step(epoch_val_loss)
        early_stopping(epoch_val_loss, model)

        if early_stopping.early_stop:
            print("Early stopping")
            break

    return train_losses, val_losses, test_losses, train_accuracies, val_accuracies, test_accuracies

num_epochs = 20

train_losses, val_losses, test_losses, train_accuracies, val_accuracies, test_accuracies = train_and_evaluate(model, train_dataloader, val_dataloader, test_dataloader, criterion, optimizer, scheduler, num_epochs, early_stopping)

epochs = range(1, len(train_losses) + 1)

plt.figure(figsize=(14, 7))

plt.subplot(1, 2, 1)
plt.plot(epochs, train_losses, label='Training Loss')
plt.plot(epochs, val_losses, label='Validation Loss')
plt.plot(epochs, test_losses, label='Testing Loss')
plt.title('Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plotting training, validation, and test accuracies
plt.subplot(1, 2, 2)
plt.plot(epochs, train_accuracies, label='Training Accuracy')
plt.plot(epochs, val_accuracies, label='Validation Accuracy')
plt.plot(epochs, test_accuracies, label='Testing Accuracy')
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

